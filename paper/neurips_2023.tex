\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\newcommand*\xor{\oplus}


\title{Codeword Diffusion Models}

% eparate the names and addresses of multiple authors: \And and \AND.

\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
}


\begin{document}

\maketitle


\begin{abstract}
Diffusion based generative models have recently shown state of the art performance in generation tasks for many different kinds of data but show the best performance for data that naturally admit a continuous representation, such as images. 
For fully discrete data such as text fully auto-regressive modeling approaches often out perform diffusion models.
One possible source of this performance disparity is that perturbations to discrete data must themselves be fundamentally discrete when transforming one symbol into another most or all of the information carried by that symbol will be destroyed all at once. 
This work explores a fully discrete diffusion method in which instead of fully transforming symbols into one another the information in each symbol may be gradually eroded. 
This is accomplished by first transforming each symbol into a binary codeword and then applying a discrete diffusion process. Because most binary patterns are not valid codewords the diffusion process can take many steps before transforming the representation for each symbol into a valid representation for another.
\end{abstract}


\section{Introduction}

Diffusion based models offer several potential advantages over auto-regressive generative models for discrete sequence generation, the minimum number of required model evaluations to generate a sample is not constrained by the length of the sample and sample quality can be traded for faster generation by adapting the generative denoising schedule.

The current state of the art generative models for discrete data, such as text, are auto-regressive models which generate serialized representations of the objects generated one symbol at a time conditioned on previously generated symbols. 
However not all discrete sequence data would be most naturally predicted in the order in which it is presented. 
As an illustrative example it is much easier to predict that 
%Auto-regressive models generating high quality quality samples from such domains usually employ specialized sub-model to reversibly transform between the image domain and a consistently serialized sequence of discrete symbols \citep{ramesh2021, lee2022} . 

A much more natural fit for the generation of non-sequential and continuous data are diffusion probabilistic models \cite{sohl-dickstein15}.
Diffusion based models have recently achieved state of the art performance in image generation \citep{ho2020, song2021, saharia2022, ramesh2022}. 
Diffusion models work by making progressive refinements of a generated sample, starting from a fixed noise distribution. 

The progressive updating of diffusion models allows for more natural handling of non-sequential relationships since the iterative updates at each time step have access to the entire corrupted sample at each time point and there is no need to condition on only specific parts of the data. 
This ability of diffusion models to make progressive updates is also very attractive with respect to textual generation or other forms of discrete data. For example consider a text generation system trained on two different sorts of text. Both texts describe the moves in a Chess game. One text has a header which lists the total number of moves in the game at the very beginning and the other file is identical but lists the number of moves at the end. Diffusion like models would have the same amount of difficulty in generating either game format, since it is equally easy to communicate information forward or backward within the textual sequence.
Whereas these two different formats present a very different kind of challenge to a fully autoregressive model. If the game length is presented at the end then the autoregressive model can learn to predict a plausible sequence of moves and count them up at the end (or rather predict the text of the move count conditioned on the game). This seems like a relatively natural way to go about generating believable samples of chess games. But if the number of moves is instead at the beginning of the text then the system must not only predict a sequence of plausible moves but must also condition those moves such that checkmate is achieved in precisely the right number of steps. In other words the system must effectively already have an idea of how the game is going to ultimately play out before the first move is even made. 

%For a human it would seem that the task 'play a game of chess and then count the number of moves' is a much easier task than 'play a game of chess consisting of exactly N moves'. So we might expect that an autoregressive model would have a much harder time with the format in which game length is specified first than when it is specified at the end. However it could easily be the case that the additional conditioning information could very well yield superior apparent sample quality, especially when the number of available training examples becomes very large.
%A situation analogous to how label conditioning tends to improve the quality of generated images.   
However we should be wary of the implicit biases of models which can, metaphorically speaking, only think in a straight line. For example a decision transformer \cite{chen2021decision} pre-trained on text where the number of moves in the game is at the beginning would implicitly sometimes be conditioned to attempt to lose the game instead of win depending on whether or not the initially generated number of moves was even or odd.

Additionally codeword based diffusion models may provide a superior way to deal with categorical predictions over extremely large vocabularies. Dense prediction heads with softmax activations become very costly in terms of numbers of parameters, amount of computation, and memory as the size of the target vocabulary increases. Categorical prediction can be done much more parameter and compute efficiently by instead predicting the digits of the binary expansion of the category index. However the interpretation of all of the bits relies on the values of the ones that came before in the index. Better predictive performance can be gotten by hierarchically predicting each bit conditioned on the previously predicted bits. In practice instead of predicting all the way down at the level of bits it is useful to make predictions over hierarchical word clusters \cite{pmlr-vR5-morin05a}. The cluster hierarchy however is likely to be somewhat arbitrary and managing this hierarchy adds significant additional code and model structure complexity. 

In codeword diffusion large vocabularies can be represented using only a logarithmic number of bits plus some small encoding overhead. Because the diffusion updates are intended to be run over several iterations complex interactions between the encoding bits may be learned without the need for any explicit hierarchy to be hardcoded into the model. Thus we can achieve the parsimony of hierarchical encoding schemes without requiring explicit management of these hierarchies. 

\section{Method}

In generative modeling the goal is to utilize samples from a data distribution $p_{\rm data}(x)$ to learn a distribution which well approximates the data distribution and that admits a known sampling method.
In generative diffusion models a forward diffusion process is first defined which progressively injects noise into an initially valid data point until the end result is essentially all noise. 
Once the forward diffusion process is defined sample trajectories can be drawn from it and models can be then be trained to predict the pattern of noise present in a partially noised training sample.  
If successful then repeatedly subtracting off a portion of the noise predicted by the model will yield a sequence of points which will tend towards the data distribution. 

\subsection{Forward Replacement Diffusion Process}

Let $x_{j,k}(t)$ denote the value at time $t$ of a sequence of bit vectors with sequence index $j$ and bit index $k$. 
When the sequence index and bit vector index are omitted operations are to be assumed to be applied element wise over the sequence and bit dimensions.
%When convenient $x_j(t)$ will be treated as equivalent to the integer whose binary expansion it corresponds to.

We define a forward diffusion process for $x(t)$ in terms of replacement events that can happen at continuously valued times.
Ordinarily gradual noise based diffusion processes are thought of in terms of small additive perturbations the effect of which accumulate over time.
In the case of binary valued data the addition of additive noise amounts to selecting a subset of bits whose values are to be flipped. 
It may therefore seem most natural to define the forward diffusion process in terms of random bit flips, however this makes the distribution of the data at any time $t$ dependent on the entire history of bit flips up to that point which unnecessarily complicates the statistical analysis of the process. 
Since perturbations to individual bits cannot be gradual it is unnecessary to think of the noise perturbations in terms of additive noise, instead we choose to model the process in terms of progressive random replacement events. 
In a replacement event where the value of a selected bit is replaced by a value randomly drawn from a binary valued noise distribution.
The replacement noise distribution will be denoted $\eta$ and unless otherwise noted $\eta \sim {\rm Bernoulli}(0.5)$.
It is important to note that, under this definition, it is possible for a replacement event to take place without the value of that bit being changed, and so replacement events sometimes corrupt the value selected bit but they can take place without any observable effect. 
But from a statistical perspective the distribution of a bit after a replacement event can be taken to be completely independent of the distribution of any other bit values and this is true even if the replacement event had no observable effect on the data. 

For training purposes it is useful to be able to draw samples from the distribution over $x(t)$ at any time $t$ conditioned on the value at $x(0)$ without needing to run a simulation of the diffusion process forward in time. 
Because the replacement noise distribution is taken to be constant over time the distribution over replaced values remains stationary at $\eta$ for any number of replacements 1 or greater.
Therefore to draw a sample from the diffusion process at time $t$ one can first draw a replacement noise value for every bit from the replacement distribution $\eta$.
Then one simply selects bits either from $x(0)$ or the samples from $\eta$ with a probability equal to the probability that at least one replacement event will have taken place by time $t$. 

If successive corruption events are assumed to be independent of each other and occur with a rate $\beta(t)$ then the distribution of replacement events to have occurred in a time interval $(t_1, t_2)$ will be Poisson distributed with a mean given by the integral of the replacement rate $\beta(t)$.
The probability for a count of exactly 0 to be drawn from a Poisson distributed variable with mean $\mu$ is given by $e^{-\mu}$. 
Using this we can calculate the cumulative probability of one or more replacement events by time $t$ as given by equation \ref{totrepfrac} and the probability of one or more replacement events occurring in the interval $(t-\Delta, t)$ via equation \ref{fdeltadef}.


\begin{equation}\label{totrepfrac}
  f_c(t) = 1 - exp \left(- \int_{0}^{t} \beta(t) dt \right)
\end{equation}

\begin{equation}\label{fdeltadef}
  f_\Delta(t) = 1 - exp \left( - \int_{t-\Delta}^t \beta(t) dt  \right)
\end{equation}


\subsection{Discretized Reverse Process}

Consider a discretized version of the reverse process beginning with a noise sample $x(T)$ and then repeatedly attempting to sample from the approximate distribution over points a short duration $\Delta$ earlier in time $p(x(t-\Delta) | x(t))$.
Denote the change between the value at the beginning and end of this interval as $\delta x_{j,k}(t) = x(t) \oplus x(t-\Delta)$.
If $\delta x(t)$ were known then $x(t-\Delta)$ can be obtained as $x(t-Delta) = x(t) \oplus \delta x(t) $.
Therefore if we had access to a good estimate of the conditional distribution over incremental changes $p(\delta x_{j,k} | x(t), t)$ then we can generate samples for $x(0)$ by repeatedly sampling from the conditional distribution over recent bit flips and then applying them as updates to the current sample. 

To evaluate this conditional distribution first note that $\epsilon(t-\Delta) \oplus \epsilon(t) = x(t-\Delta) \oplus x(t)$.
For any time $t$, $\epsilon_{j,k}(t) = 1$ if and only if there has been at least one replacement event for that bit and the most recently drawn replacement value did not match the original bit value. 
For the particular choice of replacement distribution $\nu \sim {\rm Bernoulli(0.5)}$ the chance of a match or mismatch is always 1/2 regardless of the distribution of bit values being matched, therefore $p(\epsilon_{j,k}(t) = 0) = f_c(t)/2$ and by similar reasoning $p(\epsilon_{j,k}(t-\Delta) \neq \epsilon_{j,k}(t)) = f_\Delta(t)/2$.
These probabilities can be used to calculate the probability mass which flows from both possible states for $\epsilon_{j,k}(t-\Delta)$ into both possible states of $\epsilon_{j,k}(t)$. 
We can use these probabilities to calculate the rate at which each bit would have been flipped in a forward process during the interval $(t-\Delta, t)$ given the current value of $\epsilon_{j,k}$.
For example the probability of a recent bit flip conditioned on the bit in question currently being corrupted can be calculated as such; 
$p(\epsilon_{j,k}(t-\Delta) = 0 | \epsilon_{j,k}=1) 
= p(\epsilon_{j,k}(t-\Delta) = 0, \epsilon_{j,k}=1)/p(\epsilon_{j,k}(t)=1) 
= (1-f_c(t-\Delta)/2)*f_\Delta(t)/2/((1-f_c(t-\Delta)/2)*f_\Delta(t)/2 + (1-f_\Delta(t)/2)*f_c(t-\Delta)/2)$

The true value of $\epsilon_{j,k}(t)$ will not be known during the application of a reverse update process, but we can use a learned model to estimate the probability distribution $p(\epsilon_{j,k}(t) | x(t))$ noise values conditioned on the current observed $x(t)$ and then flip each bit at a rate which is the probability weighted sum of the bit flip rates which would have occurred in the case of each condition. 
The results of constructing a reverse process in this way are detailed in equations \ref{denoisetarget} through \ref{c1mimic}. 

\begin{equation}\label{denoisetarget}
  f_\theta(x(t), t) \approx p(\epsilon_{j,k}(t)~|~x(t), t) = p(x(t) \xor x(0)~|~x(t), t)
\end{equation}

\begin{equation}\label{updatedistrib}
  \delta x_{j,k}(x(t), t) \sim {\rm Bernoulli( 
    r(t)*\left( 
    c_0(t) + (c_1(t)-c_0(t))*f_\theta(x(t), t)  
    \right)
  }
\end{equation}

\begin{equation}\label{applyupdate}
  x(t-\Delta) = x(t) \oplus \delta x_{j,k}(x(t))
\end{equation}

\begin{equation}\label{rfdelt}
  r(t) = f_\Delta(t)
\end{equation}
\begin{equation}\label{c0mimic}
  c_0(t) = \frac{f_c(t-\Delta)}{(f_\Delta(t)*f_c(t-\Delta) + (2-f_\Delta(t))*(2-f_c(t-\Delta)))}
\end{equation}
\begin{equation}\label{c1mimic}
  c_1(t) = \frac{(2-f_c(t-\Delta))}{(2-f_c(t-\Delta))*f_\Delta(t) + (2-f_\Delta(t)*f_c(t-\Delta))}
\end{equation}

Note that in this analysis the probability of each bit flip in the reverse process is being sampled independently of all other bit flips that may occur within a given reverse time step which may not be a valid approximation when the bit values are strongly interdependent.
This limitation can be overcome from a theoretical perspective by picking extremely small time steps such that there will be either 0 or 1 bit flip over the whole sample in which case this reverse sampling process effectively becomes gibbs sampling with the conditional distribution for each bit being sampled one at a time in random order. 
Smaller time steps may be necessary to precisely match the distributions of reverse and forward trajectories but in practice it is less important that the reverse generated trajectories be indistinguishable from forward trajectories and more important that the distribution over $x(0)$ be similar in both cases.


In general there is no strong reason to suppose that the best schedule for sample generation using the reverse process will also correspond to the parameters used for generating training data from the forward process. 
The parameterization of the update distribution defined in equation \ref{updatedistrib} has been chosen in order to allow for intuitive control of reverse processes to facilitate tuning for achieving good empirical performance for particular models and data distributions. Intuitively $r(t)$ controls the average rate of allowed bit flips, $c_0(t)$ controls how often to flip bits that might already be correct and $c_1(t)$ controls how often to correct bits that might be currently incorrect. 
For example a reasonable generating schedule might be $c_0(t)=0$ $c_1(t)=1$ and $r(t) = k*T/\Delta$ which would simply flip each bit proportional to its current probability of corruption with each bit being selected to have its value updated an average of $k$ times. 


%Unlike for the forward process the bit values become increasingly statistically dependent on each other over time instead of vice versa. 


\subsection{Binary Codeword Mapping}

One of the most common ways to represent discrete objects for generative modeling is to represent them as a sequence of symbols from a known vocabulary, or equivalently as a sequence of categorical variables
Usually for implementation purposes these symbols or categories are finally mapped to sequence of integers representing the index of the relevant symbol or category in some list.
For example it has become standard to begin textual modeling by partitioning the text into short substrings, called tokens, which can be mapped onto integers representing the index in the vocabulary of each token. 
Other kinds of discrete data is often subjected to a similar kind of transformation process. A user session on a website could be be represented by a sequence of document ids(usually remapped to be dense within a fixed range of indexes, similar to the tokenization indexing transform). 
An episode in a reinforcement learning problem may be chosen to be represented as a sequence of integers representing unique states and actions. 
Even data which is not typically thought of as being discrete is easily represented this way, for example although image data is usually modeled as 2D or 3D arrays of floats one could also in principle choose to model an image as a sequence of integers, either the raw pixel values in the range 0 to 255 or as the byte sequence of some appropriate serialization format such as .png, .bmp, etc. 

In order to represent a sequence of symbol indexes $s_{j}$ as a sequence of binary bit vectors $x_{j,k}$ in preparation of training a binary diffusion model one fairly straight forward way to proceed would be to convert each integer $s_{j}$ into its corresponding binary expansion. 
Such an approach can work but this representation has the disadvantage that the mapping from symbols to bit representations will be dense meaning that flipping just one bit in the representation will in general change the bit representation to another valid bit pattern corresponding to a completely different symbol.
This means that even "small" perturbations in terms of bits will completely change the apparent symbol being represented. 
Such relationships between symbols is unlikely to act as an inductive bias when the bit patterns being used are simply the index of the symbol in an arbitrarily ordered list. 

In order to make it possible to consider the flipping of single bits as a small perturbation when seen from the perspective of the space of associated symbols we propose the idea to first remap the symbol indexes to a selected set of bit patterns (or equivalently a special set of integers) such that several bits in the corresponding pattern could be corrupted without also landing on a bit pattern which is allocated to any other valid symbol. 
Such a set of bit patterns is sometimes referred to as a binary code and a member of that set is called a codeword. 
In traditional applications, such as error correction, binary codes are often required to have the property that the minimum hamming distance between valid codewords should be large. 
In this work, in principle, any mapping from symbols to unique positive integers can in principle be used as the codeword map, though efficient decoding may not be possible for arbitrary random codes.
Though, a powerful model should in principle also be able to guide the binary representations to eventually consist purely of valid symbol codes so a traditional error correcting decoder is potentially unnecessary. 

The minimum number of bits required to represent the symbol indexes as positive integers will be referred to as the symbol bitwidth and the minimum number of bits required to represent the codewords will be referred to as the codeword bitwidth.


\section{Experiments}

We consider model performance on a range of different tasks all of which utilize the same basic form.
Each task consists of; a dataset of target sample objects, a method to turn a sampled object into a sequential symbolic representation of a definite form called a task surface, a method for generating a mask which indicates which symbols in the task surface are to be admitted as context during the generation process, and some number of metrics which can be used to gauge task performance as a function of generated task surfaces.

Each task has performance evaluated on baseline approaches and variants of codeword diffusion. All approaches utilize the same basic model architecture and have their performance compared after the same number of updates performed on the same number of total basic symbols.
At training time a batch of training data is generated as a set of constant length sequences which consist of the concatenation of task surfaces separated by a special indicator which augments the basic symbol vocabulary used by the task. 

The chosen tasks have been designed such that most task documents are small relative to the total sequence length used for training. 



At training time an object is sampled from the training dataset and then transformed into its corresponding task document and mask. A  


Consider a dataset with objects of the form \{"x":1, "y":1, "operation":"+", "result":2\} which could be used to create multiple different tasks depending on the chosen sequence representation and mask. For example a task 


 consisting of samples which will be referred to as objects. 
For each task 


\subsection{Diffusion Models} 

The task of generative modeling is to use a given set of samples drawn from an unknown target distribution to generate new samples that could plausibly have been drawn from the target distribution. The dominant modern generative model approaches 

Diffusion probabilistic models \citep{sohl-dickstein15} attempt to slowly change pure noise samples into samples which approximate those drawn from a target distribution. This differs from GANs or VAEs which attempt to learn to turn noise into data samples directly in a single pass. Turning samples from a noise distribution into samples from a target distribution may be very hard. Diffusion models leverage the fact that processes which go the other way and turn data samples into pure noise are very easy to design. For diffusion probabilistic models the noise 

The diffusion process starts by sampling a point $x^0$ from the target distribution $q(x^0)$ and at each time step a new sample $x^{t+1}$ is drawn from a distribution conditioned on the previous point in the sequence $q(x^{t+1} | x^{t}, \beta(t))$.
 where $\beta(t)$ is a parameter controlling how much noise is injected. 
For example in a continuous diffusion processes $\beta(t)$ may represent the variance of the Gaussian noise which is to be added to $x^t$. For a discrete distribution $\beta(t)$ could represent the probability of some discrete change, like a bit flip or a transition between discrete categories. 
In the limit of many transitions $T$ the ultimate distribution $q(x^T)$ can be made to approximate some desired simple known distribution. 
For example in the case of additive Gaussian noise the central limit theorem ensures eventual convergence of $x^t$ to a Gaussian distribution.   

Diffusion models work by attempting to approximate the distribution of a reverse process $q(x^{t-1} | x^t, \beta(t)) \approx f_\theta(x^t)$.
The function $f_\theta$ may be parameterized in any convenient manner and is typically a deep neural network trained via gradient descent. 
Pairs of samples $x^{t}$ and $x^{t+1}$ can be easily drawn from the forward diffusion process and then used as training data to optimize the parameters $\theta$.

Rather than attempting to predict only the incremental change $x^{t-1}$ from $x^t$ one can build models to predict either the fully denoised sample $x^0$ or the full noise vector $\epsilon^t = x^t - x^0$. Such models can still be used to generate incremental reverse trajectories which are optima of an analogous variational objective as that proposed in the original probabilistic diffusion approach \cite{ho2020, song2021}. 

%For certain choices of diffusion noise distribution it is possible to directly sample from the conditional distribution $p(x^t | x^0)$ without needing to carry out the intermediate diffusion steps which can greatly increase the efficiency of training sample generation.

\subsection{Binary Code Diffusion}

The symbol indexes are often the direct objects of the modeling process, both for input and for sample generation. The present work adds one additional layer of transformation in which the symbol indexes are transformed into code words. The transformation into code words serves an analogous purpose in binary code diffusion models as a one hot encoding transform might serve in a more traditional categorical prediction model.

Depending on the context it may be simpler to think of the code words either as integer values or in terms of the binary expansion of those integers. For example a simple one hot encoding can be thought of in terms of code words as being either binary strings with exactly a single non-zero entry or equivalently in terms of integers of the form $2^m$. 
A more information dense code could be achieved by taking the sets of binary strings with at most two ones or the integers of the form $2^m + 2^n$. A one hot encoding with 1,000 bits could be replaced by such an encoding with just 45 bits. 
A very information dense encoding can be achieved by simply letting the symbol indexes be equal to their code word representations, encoding a vocabulary of 1,000 symbols this way requires only $log_2(1000) \approx 10$ bits. 

The diffusion process then proceeds by randomly corrupting bits in the encoded representation of the data until most or all bits have been replaced by random noise. In normal usage the terms "flipped bit" and "corrupted bit" would usually have the same meaning as each other. However in this work a corruption event need not necessarily flip the value of the corrupted bit. A corruption event simply replaces the current value of a bit with a new randomly generated value, which may or may not correspond to the original bit value. Formulated this way corruption events completely destroy the dependency of bit values on previous 

The codes which correspond to the one hot encoding would yield generative models which are very similar to the typical method of predicting likelihoods per category.
Usually this is the final step in most data preprocessing pipelines. The present work adds one additional optional step in which the 

symbols from a fixed vocabulary. Once the  and the tokens are then mapped to integers which index into the vocabulary. The present work adds one additional step in which the symbol indexes are transformed into code words. The code words can also be thought of as being integers within some fixed range but the set of valid code words may be strategically chosen to be very small compared to. 

in which the symbol indexes are further transformed into specially chosen integers 

For the purposes of this work it is helpful to partition the sample generating process into three different components, denoising, decoding, and deserialization. For this work only the denoising component of the generator is trained, the decoding and deserialization 

The code words are then corrupted by randomly replacing the values of bits with randomly selected values. 

In contrast to the usual case of affairs in generative modeling. 
 a binary representation. The present work mostly focuses on models where both the data and the diffusion process are discrete, though one can apply a continuous diffusion process to originally discrete binary data as is done in /citet{chen2023analog}.

In contrast to most generative modeling processes the raw data is not originally 
The input data is not assumed to initially consist of binary or binomial basic variables, instead the basic symbols of an input sequence are first mapped one-to-one onto a chosen set of encoding integers, called codewords. 
To differentiate codeword transformed inputs by $x$ and when necessary we will denote the raw input symbols indexes by $u$. 
Ca
Call the total number of bits which are necessary to represent the raw integers $u$ the bit width of the data and the number of bits required to represent the codes in the codebook the bit width of the code. The code bit width must always be at least equal to the data bit width 
Note that in general there will be no simple relationship between the bits of $x$ and the bits of $u$ and the number of bits used to encode $x^t_{i,j}$ must be larger than or equal to the number of bits used to encode the symbol indexes $u_{i,j}$.

\subsubsection{Codeword Selection}

The choice of mapping of symbol to codewords may be arbitrary, subject only to the constraint that each unique input symbol is mapped to a unique codeword. An algorithmic decoder which inverts this process and turns arbitrary messages back into valid code words is not strictly necessary, since a well trained diffusion model should do this automatically as part of the generation process, though such a decoding scheme to ensure the final bit patterns are always valid codewords can only improve performance. 


For implementation purposes we will explore only codes for which the total required encoding bits can be represented by a int32. Furthermore for ease of implementation the sign bit has been left unused in the experiments in this paper and so we consider only codes of total bit width 31 and less. 

For this work codes with a relatively small number of bits are chosen by simply randomly generating a small lookup table, called the codebook, of unique integers less than $2^k$. Mapping to codewords is done by indexing into the codeword table and decoding is done by indexing into a lookup table which enumerates the nearest codeword for every integer less than $2^k$. This technique allows easy experimentation with any choice of codebook without requiring a known algorithmic decoder. This technique works well for codes with a small number of bits but is burdensome when dealing with codes with millions of possible codewords. For this purpose when dealing with a large input vocabulary a compound block code is generated. To encode each input symbol index is first split into a multi-index with sub indexes within the range of the number of codewords used to encode each block. These sub indexes are then encoded and the binary representations of the resulting codewords concatenated. Decoding is done by decoding each block of bits to get the relevant sub indexes and then recombining them into the flat input symbol index. 

A case of special interest is the transformation of unicode codepoints into binary codewords for the purposes of applying bitwise diffusion. The usual way of encoding unicode into a binary representation is to use the UTF-8 encoding standard. However UTF-8 uses variable length codewords which is inconsistent with the implicit assumption made in this work that certain blocks of bits in the intermediate representations $x^t$ can be consistently mapped to a single symbol in the uncorrupted inputs. There are $1,112,064=2^{11}*3*181$ valid unicode codepoints. Using the combined block code approach above one can split the unicode codepoint into two block codes with vocabulary size $2^{10}=1024$, and 2*3*181=1086 and then assign codewords with any number of bits greater than 10 to each block. Because of implementation considerations it has proven convenient to use int32 values for encoding and to leave the sign bit unused. This yields 31 encoding bits to split between the two blocks and it makes sense to assign 15 bits to the block code with 1,024 codewords and 16 bits to the block code with 1,086 codewords. 

%There are many reasonable ways in which to define a consistent diffusion process over this space. Perhaps the most intuitive is to simply pick some small possibly time dependent bit flip rate $\beta(t)$ and then to randomly toggle the value of each bit in a small time step $dt$ with probability $\beta(t)dt$. As $\int \beta(t)dt$  becomes large this diffusion process will converge towards an uncorrelated binomial distribution. However for small and intermediate times the parity of $\int \beta(t)dt$ is a good predictor of $\eps^{(t)}$, which is an undesirable learning signal. Moreover efficiently sampling from such a distribution at any given time $t$ without simulating intermediate time steps is somewhat non-trivial. 

\subsubsection{Corruption Noise Sampling}

It is desirable to choose a diffusion process for which we can efficiently sample from the posterior distribution for any time $t$ conditioned on a data point $x^0$ without carrying out any intervening diffusion steps. 
Consider a diffusion process with some incremental probability of a corruption event $\beta(t)dt$ for each bit and each time step $dt$. When a corruption event occurs the current value of the corrupted bit is replaced by a distribution drawn from a noise distribution, for example a Bernoulli(1/2) distribution or a constant distribution of all zeros or all ones. Sampling from the posterior of such a process can be broken into two tasks, calculating the cumulative probability that no corruption event has happened for each bit, and then carrying out one corruption event for each bit that has been corrupted irrespective of how many times on average that bit may have been corrupted previously. 
Calculating an exact closed form for the probability of no past corruption event given a specific corruption rate $\beta(t)$ may in general be burdensome. However once the average uncorrupted fraction is known generating a sample from the posterior is very easy. However there is no need to specify the diffusion process in terms of a corruption rate $\beta(t)$, we can instead simply define the corruption process in terms of the probability that a bit remains uncorrupted at time t, we denote this probabilty as $\gamma(t)$

To sample directly from the posterior of this process we first draw a time uniformly from the interval 0 to $T$, $t ~ \mathcal{u}(0, T)$ and then corrupt a fraction $1-\gamma(t)$ of bits. Note that corrupting a bit may or may not cause the value of that bit to change. Samples drawn from the noise distribution may happen to have the same value as the bits that they are corrupting in which case the corruption action has no observable effect. 

Once a sample $x^t \sim q(x^t | x^0, t)$ has been drawn the target corruption noise may be evaluated by applying a bitwise exclusive OR operation which we denote by the symbol $\xor$.

\begin{equation}
  \epsilon^t = x^0 \xor x^t
\end{equation}


\subsubsection{Denoising Model}

Either of $\epsilon^t$ or $x^0$ or both may then be used as a prediction target for training a denoising model $f(x^t, t, \theta)$. Unlike in the continuous case the kind of value output by the denoising model may not be of the same kind as the input $x^t$.  
In the most common case for this work the input $x^t$ is a sequence of integers and the output of $f$ is a set of bit flip probabilities over the binary representations of those integers, as per equation \ref{denoiseapprox}. 


\begin{equation}\label{denoiseapprox}
  f_{i,j,k}(x^t, t, \theta) \approx p(x_{i,j,k}^0 \xor x_{i,j,k}^t) = p(\epsilon_{i,j,k}^t)
\end{equation}

The inputs to the model $x^t_{i,j}$ are integers between 0 and $2^K$ for a total codeword bit width of $K$, and the outputs are probability distributions over bit flip probabilities. In order to combine these bit flip probabilities with the current sample estimate the bit flip probabilities can be sampled from and then encoded as the bits of an integer, resulting in a discrete noise estimate $\hat{\epsilon}^t$ as in equation \ref{samplenoise}. Note that the index $k$ represents the index of the kth bit of an integer on the left hand side of equation \ref{samplenoise}, and represents a channel index of a vector of probabilities on the right hand side.  

\begin{equation}\label{samplenoise}
  \hat{\epsilon}_{i,j,k} \sim {\rm Bernoulli}(f_{i,j,k}(x^t, t, \theta))
\end{equation}

The $\xor$ operation is its own inverse and so at each time step we can evaluate a prediction for a fully denoised sample via $\hat{\epsilon}^t \xor x^t = \hat{x}^0$. However applying the maximal updates causes the generated samples to quickly fall into a stable configuration and most of the update cycles become wasted. To take full advantage of the incremental nature of the diffusion process the updates need to be gated so that only a relatively small number of bits are likely to change with each update step. To carry out this function we include a gating function $g$ which controls the fraction of bits which are allowed to flip as a function of the current sample information content as in equation \ref{gatedupdate}.

\begin{equation}\label{gatedupdate}
  \hat{x}^{t-\delta} = \hat{x}^t \xor ({\rm Bernoulli}(g(\gamma(t))) * f_\theta(x^t, t, \theta))
\end{equation}

Although the gating function $g$ for the reverse process fulfills a similar function as the corruption rate $\beta(t)$ does for the forward process we cannot use the later to fulfill the role of the former. 
To see this consider the situation in which we are attempting to reverse a process where we corrupt 1\% of bits at each forward diffusion step. For the very first diffusion step all points have never been previously corrupted and so exactly 1\% of bits are corrupted for the first time in the first forward step. Contrast this with the situation where in a reverse process sample generation 99\% of bits have reasonable values and we need to correct only the last 1\%. If the update masks cover only a random 1\% for each update then instead of taking an average of 1 step to correct it will take an average of 10,000 updates before all of the bit that require correction are selected in one of the update masks. 

Instead of using a random gating function it is tempting to simply use the probabilities of the denoising model directly to guess which bits should be flipped and to prioritize those with high probabilities first. Unfortunately it is not correct to interpret the bit flip probabilities of the diffusion model as the likelihood that any particular bit will ultimately become flipped. Rather they represent those flip probabilities {\it conditional} on the current values of all other bits. Thus especially early on in the sample generation process high bit flip probabilities may not represent high confidence estimates of final values, but rather may represent clusters of bit patterns which are mutually inconsistent with each other. 
Thus the chances that either cluster of bits should be flipped is very high but if both are toggled in the same update step then the chances that those bits will be flipped may continue to remain high resulting in bit values oscillate back and forth instead of converging to a self consistent configuration. 

One intuitive candidate for the gating function is $g(\gamma(t)) = \gamma(t)$ in which the fraction of allowed updates is simply proportional to the current estimate of the uncorrupted data fraction. At an intuitive level this would slowly increase the fraction of bits that can be corrected at the same rate implied by the diffusion process. If the estimated corruption for each bit at each step is always high probability and stable over time then each bit is flipped on average only once and in accord with a schedule that directly mimics the diffusion schedule implied by $\gamma(t)$. If on the other hand the denoising model remains uncertain of the current corrupting noise then the model is allowed to continue to repeatedly flip any bits for which the uncertainty remains high, all the way to the final update in which lets through the full maximum likelihood or sampled update.

The gating function clearly fulfills a role in these models which is analogous to the update step size in continuous diffusion models. 
By analogy with update rules for continuous diffusion processes then we might suggest the function $g(\gamma(t)) \sim \sqrt{\gamma(t)}$.   
This gating function allows updating of a greater fraction of bits early in the sampling process and is linear in $\gamma(t)$ close to the end of the sampling process.


\subsubsection{Circular Shift Embeddings}

The denoising model input data $x^t$ may represent any integer up to $2^K$ where K is the total bit width of the chosen codewords. If K is small then we may easily use the standard methods for categorical inputs by use of a learned vector embedding table lookup as an input layer for the denoising network. However since we will be considering models where this implies a categorical vocabulary of a cardinality as high as $2^{31}$ a direct embedding lookup will not always be feasible. 

To address this issue we propose the use of a circular bit shift embeddings. In a circular bit shift embedding blocks of $m$ bits from $x$ are used as the index into a collection of learned embedding lookups, one for each shift selected by the practitioner. 
The lookup indexes are calculated by carrying out circular bit shift operations to put the desired block of bits into the lowest order positions and the integer value corresponding to those bits are read off by a modulus operation. 
Each encoding bit may optionally participate in multiple different blocks and lookups. A circular bit shift operation is used instead of right shifting so that the least and most significant bits can both participate in as many contiguous blocks of bits as the interior encoding bits. 
This method provides a much higher rank representation of the individual bit values than simply treating them as linear inputs while also keeping the total parameter and memory cost low even for very high cardinality inputs.

For example consider a situation where the total encoding bit width is $K=5$ and we have chosen a sub encoding bit width $m=3$ with circular shifts of 0, 2, and 4. The first three bits would be used as an index of an embedding lookup table of size $2^m=8$ the third through 5th bits would be used as the index to a second lookup embedding of the same size and the third shift of 4 would result in a wrap around so that the last bit and the first 2 bits would be combined into an index for a third lookup. In this scenario each bit participates in exactly 2 indexing blocks and so has equal impact on the output representation vectors. 


\section{Experiments}

\subsection{Sudoku}

Sudoku represents an excellent test case for learning discrete generative models. The relationships between symbols is complex enough to represent a non-trivial learning challenge while also being small enough to admit much more rapid experimentation than is possible with natural language or image data.
Additionally there is a clear and easily evaluated concept of sample quality, namely whether or not the generated samples follow the rules of sudoku.  
But the ideal properties of a sudoku grid generative model are easy to determine from theoretical considerations and any given generative model can easily be compared against them with relatively minimal computation. 
Firstly, the number of rule violations (non unique values in row, column, or 3x3 boxes) should be as low as possible, that is the generated samples should be valid sudoku solution grids. 
Secondly, each possible valid grid should be generated with roughly equal probability. 
There are approximately $6.7x10^{21}$ valid solved 9x9 sudoku grids which could be turned into more than $10^{38}$ possible valid puzzles even when restricting consideration only to puzzles with the minimum possible number of clues \cite{mcguire201316clue}. 

We compare the generative performance of an autoregressive transformer, with discrete codeword diffusion, and continuous diffusion on analog bits \cite{chen2023analog}. 
Sample quality is measured in terms of the number of logical violations of the rules of Sudoku, measured in terms of hte number of non-unique values per row, column, and 3x3 box, as well as vocabulary violations which occur when a symbol outside of the vocabulary "123456789" is generated.

Once a generative model is trained on existing board examples it can also be used as a sudoku solver by conditioning the updates on the known entries provided in a solvable puzzle.  
Any good solver requires having complete access to all the known entries before generating any entries in the solution. For an auto-regressive generation solution this becomes a problem when the input data is presented only as pre-solved boards since there is no simple mechanism to directly condition on its own future outputs, though search methods could be employed. 
For the diffusion based models solving a puzzle is much easier than generating a puzzle since a valid puzzle prompt cannot have any self inconsistencies in the conditioning values, something that must be achieved by the diffusion model itself in de-novo generation.


% \begin{table}
%   \caption{Sample table title}
%   %\label{sample-table}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
% \end{table}


\section{Related Work}

\subsection{Relation to Analog Bit Diffusion}

The proposed method is strongly similar to the method of analog bit diffusion proposed by \cite{chen2023analog}. In the method of analog bits each bit in a binary data representation is promoted to a floating point value and continuous diffusion with a Gaussian diffusion process is applied. In \cite{chen2023analog} other binary representations than the default ascii byte representation were considered; gray, random, and one-hot. From the perspective of this work the gray and random binary encodings consist of the same codebook as the usual dense ascii encoding and so differ only in the way in which input symbols are mapped to codewords not in the choice of code itself. While the one-hot encoding does correspond to a   technically a valid codebook its capacity is so small it would not usually be considered in the same type of encoding as the codes considered in this work.    
Any codeword transformation pre processing step is consistent with analog bit diffusion. 

\subsection{Relationship to Structured Denoising Diffusion}

Codeword diffusion can be seen as a kind of generalization of the structured denoising diffusion of \cite{austin2021}. In the structured diffusion approach the data is encoded as a sequence of categorical variables and the diffusion process is defined in terms of probabilistic transitions between the implied categories.  In that approach the target prediction categories for denoising and the possible input categories were selected to be drawn largely from the same possible vocabulary. One exception was a special mask token which was allowed to be used for noise transitions but which may not be a valid target category. 

Codeword diffusion is mathematically similar to a structured diffusion process in which the space of possible categories over which diffusion may occur is much larger than the space of possible target categories, and each update is allowed to affect all symbols simultaneously. The explicit transition probability matrices between these categories implied by the bit corruption process would be in many cases too large to evaluate explicitly and so a direct implementation of codeword diffusion using the techniques of \cite{austin2021} may not be feasible in the general case. 
%talk about the gating function?

\section{Conclusion}

Codeword diffusion is schweet!

% \subsection{Retrieval of style files}


% The \LaTeX{} style file contains three optional arguments: \verb+final+, which
% creates a camera-ready copy, \verb+preprint+, which creates a preprint for
% submission to, e.g., arXiv, and \verb+nonatbib+, which will not load the
% \verb+natbib+ package for you in case of package clash.


% \paragraph{Preprint option}
% If you wish to post a preprint of your work online, e.g., on arXiv, using the
% NeurIPS style, please use the \verb+preprint+ option. This will create a
% nonanonymized version of your work with the text ``Preprint. Work in progress.''
% in the footer. This version may be distributed as you see fit, as long as you do not say which conference it was submitted to. Please \textbf{do
%   not} use the \verb+final+ option, which should \textbf{only} be used for
% papers accepted to NeurIPS. 


% At submission time, please omit the \verb+final+ and \verb+preprint+
% options. This will anonymize your submission and add line numbers to aid
% review. Please do \emph{not} refer to these line numbers in your paper as they
% will be removed during generation of camera-ready copies.


% The file \verb+neurips_2023.tex+ may be used as a ``shell'' for writing your
% paper. All you have to do is replace the author, title, abstract, and text of
% the paper with your own.


% The formatting instructions contained in these style files are summarized in
% Sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.


% \section{General formatting instructions}
% \label{gen_inst}


% The text must be confined within a rectangle 5.5~inches (33~picas) wide and
% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).  Use 10~point
% type with a vertical spacing (leading) of 11~points.  Times New Roman is the
% preferred typeface throughout, and will be selected for you by default.
% Paragraphs are separated by \nicefrac{1}{2}~line space (5.5 points), with no
% indentation.


% The paper title should be 17~point, initial caps/lower case, bold, centered
% between two horizontal rules. The top rule should be 4~points thick and the
% bottom rule should be 1~point thick. Allow \nicefrac{1}{4}~inch space above and
% below the title to rules. All pages should start at 1~inch (6~picas) from the
% top of the page.


% For the final version, authors' names are set in boldface, and each name is
% centered above the corresponding address. The lead author's name is to be listed
% first (left-most), and the co-authors' names (if different address) are set to
% follow. If there is only one co-author, list both author and co-author side by
% side.


% Please pay special attention to the instructions in Section \ref{others}
% regarding figures, tables, acknowledgments, and references.


% \section{Headings: first level}
% \label{headings}


% All headings should be lower case (except for first word and proper nouns),
% flush left, and bold.


% First-level headings should be in 12-point type.


% \subsection{Headings: second level}


% Second-level headings should be in 10-point type.


% \subsubsection{Headings: third level}


% Third-level headings should be in 10-point type.


% \paragraph{Paragraphs}


% There is also a \verb+\paragraph+ command available, which sets the heading in
% bold, flush left, and inline with the text, with the heading followed by 1\,em
% of space.


% \section{Citations, figures, tables, references}
% \label{others}


% These instructions apply to everyone.


% \subsection{Citations within the text}


% The \verb+natbib+ package will be loaded for you by default.  Citations may be
% author/year or numeric, as long as you maintain internal consistency.  As to the
% format of the references themselves, any style is acceptable as long as it is
% used consistently.


% The documentation for \verb+natbib+ may be found at
% \begin{center}
%   \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
% \end{center}
% Of note is the command \verb+\citet+, which produces citations appropriate for
% use in inline text.  For example,
% \begin{verbatim}
%    \citet{hasselmo} investigated\dots
% \end{verbatim}
% produces
% \begin{quote}
%   Hasselmo, et al.\ (1995) investigated\dots
% \end{quote}


% If you wish to load the \verb+natbib+ package with options, you may add the
% following before loading the \verb+neurips_2023+ package:
% \begin{verbatim}
%    \PassOptionsToPackage{options}{natbib}
% \end{verbatim}


% If \verb+natbib+ clashes with another package you load, you can add the optional
% argument \verb+nonatbib+ when loading the style file:
% \begin{verbatim}
%    \usepackage[nonatbib]{neurips_2023}
% \end{verbatim}


% As submission is double blind, refer to your own published work in the third
% person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our
% previous work [4].'' If you cite your other papers that are not widely available
% (e.g., a journal paper under review), use anonymous author names in the
% citation, e.g., an author of the form ``A.\ Anonymous'' and include a copy of the anonymized paper in the supplementary material.


% \subsection{Footnotes}


% Footnotes should be used sparingly.  If you do require a footnote, indicate
% footnotes with a number\footnote{Sample of the first footnote.} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches (12~picas).


% Note that footnotes are properly typeset \emph{after} punctuation
% marks.\footnote{As in this example.}


% \subsection{Figures}


% \begin{figure}
%   \centering
%   \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%   \caption{Sample figure caption.}
% \end{figure}


% All artwork must be neat, clean, and legible. Lines should be dark enough for
% purposes of reproduction. The figure number and caption always appear after the
% figure. Place one line space before the figure caption and one line space after
% the figure. The figure caption should be lower case (except for first word and
% proper nouns); figures are numbered consecutively.


% You may use color figures.  However, it is best for the figure captions and the
% paper body to be legible if the paper is printed in either black/white or in
% color.


% \subsection{Tables}


% All tables must be centered, neat, clean and legible.  The table number and
% title always appear before the table.  See Table~\ref{sample-table}.


% Place one line space before the table title, one line space after the
% table title, and one line space after the table. The table title must
% be lower case (except for first word and proper nouns); tables are
% numbered consecutively.


% Note that publication-quality tables \emph{do not contain vertical rules.} We
% strongly suggest the use of the \verb+booktabs+ package, which allows for
% typesetting high-quality, professional tables:
% \begin{center}
%   \url{https://www.ctan.org/pkg/booktabs}
% \end{center}
% This package was used to typeset Table~\ref{sample-table}.


% \begin{table}
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \subsection{Math}
% Note that display math in bare TeX commands will not create correct line numbers for submission. Please use LaTeX (or AMSTeX) commands for unnumbered display math. (You really shouldn't be using \$\$ anyway; see \url{https://tex.stackexchange.com/questions/503/why-is-preferable-to} and \url{https://tex.stackexchange.com/questions/40492/what-are-the-differences-between-align-equation-and-displaymath} for more information.)

% \subsection{Final instructions}

% Do not change any aspects of the formatting parameters in the style files.  In
% particular, do not modify the width or length of the rectangle the text should
% fit into, and do not change font sizes (except perhaps in the
% \textbf{References} section; see below). Please note that pages should be
% numbered.


% \section{Preparing PDF files}


% Please prepare submission files with paper size ``US Letter,'' and not, for
% example, ``A4.''


% Fonts were the main cause of problems in the past years. Your PDF file must only
% contain Type 1 or Embedded TrueType fonts. Here are a few instructions to
% achieve this.


% \begin{itemize}


% \item You should directly generate PDF files using \verb+pdflatex+.


% \item You can check which fonts a PDF files uses.  In Acrobat Reader, select the
%   menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
%   also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
%   available out-of-the-box on most Linux machines.


% \item \verb+xfig+ "patterned" shapes are implemented with bitmap fonts.  Use
%   "solid" shapes instead.


% \item The \verb+\bbold+ package almost always uses bitmap fonts.  You should use
%   the equivalent AMS Fonts:
% \begin{verbatim}
%    \usepackage{amsfonts}
% \end{verbatim}
% followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or \verb+\mathbb{C}+
% for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You can also use the following
% workaround for reals, natural and complex:
% \begin{verbatim}
%    \newcommand{\RR}{I\!\!R} %real numbers
%    \newcommand{\Nat}{I\!\!N} %natural numbers
%    \newcommand{\CC}{I\!\!\!\!C} %complex numbers
% \end{verbatim}
% Note that \verb+amsfonts+ is automatically loaded by the \verb+amssymb+ package.


% \end{itemize}


% If your file contains type 3 fonts or non embedded TrueType fonts, we will ask
% you to fix it.


% \subsection{Margins in \LaTeX{}}


% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the
% figure width as a multiple of the line width as in the example below:
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.pdf}
% \end{verbatim}
% See Section 4.4 in the graphics bundle documentation
% (\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})


% A number of width problems arise when \LaTeX{} cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command when
% necessary.


% \begin{ack}
% Use unnumbered first level headings for the acknowledgments. All acknowledgments
% go at the end of the paper before the list of references. Moreover, you are required to declare
% funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
% More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2023/PaperInformation/FundingDisclosure}.


% Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
% \end{ack}



% \section{Supplementary Material}

% Authors may wish to optionally include extra information (complete proofs, additional experiments and plots) in the appendix. All such materials should be part of the supplemental material (submitted separately) and should NOT be included in the main submission.


% \section*{References}

% References follow the acknowledgments in the camera-ready paper. Use unnumbered first-level heading for
% the references. Any choice of citation style is acceptable as long as you are
% consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
% when listing the references.
% Note that the Reference section does not count towards the page limit.
% \medskip


% {
% \small


% [1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
% connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
% (eds.), {\it Advances in Neural Information Processing Systems 7},
% pp.\ 609--616. Cambridge, MA: MIT Press.


% [2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
%   Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
% TELOS/Springer--Verlag.


% [3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
% recall at excitatory recurrent synapses and cholinergic modulation in rat
% hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
% }

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}