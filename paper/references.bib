
improved vector quantized diffusion
https://arxiv.org/abs/2205.16007


@inproceedings{austin2021,
 author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {17981--17993},
 publisher = {Curran Associates, Inc.},
 title = {Structured Denoising Diffusion Models in Discrete State-Spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf},
 volume = {34},
 year = {2021}
}

@misc{chen2021decision,
      title={Decision Transformer: Reinforcement Learning via Sequence Modeling}, 
      author={Lili Chen and Kevin Lu and Aravind Rajeswaran and Kimin Lee and Aditya Grover and Michael Laskin and Pieter Abbeel and Aravind Srinivas and Igor Mordatch},
      year={2021},
      eprint={2106.01345},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chen2023analog,
      title={Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning}, 
      author={Ting Chen and Ruixiang Zhang and Geoffrey Hinton},
      year={2023},
      eprint={2208.04202},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{hoogeboom2021argmax,
      title={Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions}, 
      author={Emiel Hoogeboom and Didrik Nielsen and Priyank Jaini and Patrick Forr√© and Max Welling},
      year={2021},
      eprint={2102.05379},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{sudokudata1m,
  author = {Kyubyong Park},
  title = {{1 million sudoku games}},
  howpublished = "\url{https://www.kaggle.com/datasets/bryanpark/sudoku?resource=download}",
  year = {2017}, 
  note = "[Online; accessed April 18, 2023]"
}

autoregressive image generation by predicting quantized residuals
particularly interesting because it is sort of a clever way to let an auto-regressive model to go back and change its mind about something by generating a sequence of updates instead of directly generating the end result.
@misc{lee2022,
      title={Autoregressive Image Generation using Residual Quantization}, 
      author={Doyup Lee and Chiheon Kim and Saehoon Kim and Minsu Cho and Wook-Shin Han},
      year={2022},
      eprint={2203.01941},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{https://doi.org/10.13140/rg.2.2.26493.64480,
  doi = {10.13140/RG.2.2.26493.64480},
  
  url = {https://rgdoi.net/10.13140/RG.2.2.26493.64480},
  
  author = {{Mengchun Zhang} and Qamar, Maryam and {Taegoo Kang} and Jung, Yuna and {Chenshuang Zhang} and Bae, Sung-Ho and {Chaoning Zhang}
},
  
  language = {en},
  
  title = {A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material},
  
  publisher = {Unpublished},
  
  year = {2023}
}


@InProceedings{pmlr-vR5-morin05a,
  title = 	 {Hierarchical Probabilistic Neural Network Language Model},
  author =       {Morin, Frederic and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics},
  pages = 	 {246--252},
  year = 	 {2005},
  editor = 	 {Cowell, Robert G. and Ghahramani, Zoubin},
  volume = 	 {R5},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--08 Jan},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/r5/morin05a/morin05a.pdf},
  url = 	 {https://proceedings.mlr.press/r5/morin05a.html},
  note =         {Reissued by PMLR on 30 March 2021.}
}


@misc{nachmani2021zeroshot,
      title={Zero-Shot Translation using Diffusion Models}, 
      author={Eliya Nachmani and Shaked Dovrat},
      year={2021},
      eprint={2111.01471},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


this is the dall-e paper
@misc{ramesh2021,
      title={Zero-Shot Text-to-Image Generation}, 
      author={Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
      year={2021},
      eprint={2102.12092},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

this is the dall-e 2 paper which switched to diffusion models
@misc{ramesh2022,
      title={Hierarchical Text-Conditional Image Generation with CLIP Latents}, 
      author={Aditya Ramesh and Prafulla Dhariwal and Alex Nichol and Casey Chu and Mark Chen},
      year={2022},
      eprint={2204.06125},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

google test to image generator
@misc{saharia2022,
      title={Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding}, 
      author={Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily Denton and Seyed Kamyar Seyed Ghasemipour and Burcu Karagol Ayan and S. Sara Mahdavi and Rapha Gontijo Lopes and Tim Salimans and Jonathan Ho and David J Fleet and Mohammad Norouzi},
      year={2022},
      eprint={2205.11487},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{sohl-dickstein15,
  title = 	 {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2256--2265},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}

DDIM paper
@inproceedings{song2021,
title={Denoising Diffusion Implicit Models},
author={Jiaming Song and Chenlin Meng and Stefano Ermon},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=St1giarCHLP}
}